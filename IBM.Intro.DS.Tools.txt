The Data Science Task Categories are:

Data Management

Data Integration and Transformation

Data Visualization

Model Building

Model Deployment

Model Monitoring and Assessment

Data Science Tasks support the following:

Data Asset Management

Code Asset Management

Execution Environments

Development Environments

MySQL, PostgreSQL, MongoDB, Apache CouchDB, Apache Cassandra, Hadoop File System, Ceph, and elastic search are data management tools.

Data integration and transformation tools are Apache AirFlow, KubeFlow, Apache Kafka, Apache Nifi, Apache SparkSQL, and NodeRED.

Data Visualization tools are Pixie Dust, Hue, Kibana, and Apache Superset.

Model deployment tools are Apache PredictionIO, Seldon, Kubernetes, Redhat OpenShift, Mleap, TensorFlow service, TensorFlow lite, and TensorFlow dot JS.  

Model monitoring tools are ModelDB, Prometheus, IBM AI Fairness 360, IBM Adversarial Robustness 360 Toolbox, and IBM AI Explainability 360.

Code asset management tools are Git, GitHub, GitLab, and Bitbucket. 

Data asset management tools are Apache Atlas, ODPi Egeria, and Kylo.

Data management tools are Oracle Database, Microsoft SQL Server, and IBM Db2.

Data integration tools are provided by Informatica PowerCenter, and IBM Infosphere DataStage.

Other products are from SAP, Oracle, SAS, Talend, Microsoft, and Watson Studio Desktop.

Model building tools are SPSS Modeler, and SAS enterprise miner.

Informatica and IBM provide data asset management tools.

Watson Studio and Watson Open Scale are fully integrated tool covering the data science life cycle.

In data management, with some exceptions, a software-as-a-service (SaaS) version of existing open-source and commercial tools exists.

Two commercial data integration tools widely used are Informatica Cloud Data Integration and IBM’s Data Refinery.

Two examples of cloud-based data visualization tools are Datameer and IBM’s Cognos Business Intelligence Suite.

Watson Machine Learning can build models.



You should select a language to learn depending on your needs, the problems you are trying to solve, and whom you are solving them for.

The popular languages are Python, R, SQL, Scala, Java, C++, and Julia.

For data science, you can use Python's scientific computing libraries like Pandas, NumPy, SciPy, and Matplotlib. 

Python can also be used for Natural Language Processing (NLP) using the Natural Language Toolkit (NLTK). 

Python is open source, and R is free software. 

R language’s array-oriented syntax makes it easier to translate from math to code for learners with no or minimal programming background.

SQL is different from other software development languages because it is a non-procedural language.

SQL was designed for managing data in relational databases. 

If you learn SQL and use it with one database, you can apply your SQL knowledge with many other databases easily.

Data science tools built with Java include Weka, Java-ML, Apache MLlib, and Deeplearning4.

For data science, popular program built with Scala is Apache Spark which includes Shark, MLlib, GraphX, and Spark Streaming.

Programs built for Data Science with JavaScript include TensorFlow.js and R-js.

One great application of Julia for Data Science is JuliaDB.



Libraries usually contain built-in modules that provide different functionalities.

You can use data visualization methods to communicate with others and display meaningful results of an analysis. 

For machine learning, the Scikit-learn library contains tools for statistical modeling, including regression, classification, clustering, and so on.

Large-scale production of deep-learning models use TensorFlow, a low-level framework. 

Apache Spark is a general-purpose cluster-computing framework that allows you to process data using compute clusters.

An application programming interface (API) allows communication between two pieces of software.

API is the part of the library you see while the library contains all the components of the program. 

REST APIs allow you to communicate through the internet and take advantage of resources like storage, data, artificially intelligent algorithms, and much more.

Open data is fundamental to Data Science.

Community Data License Agreement makes it easier to share open data.

The IBM Data Asset eXchange (DAX) site contains high-quality open data sets.

DAX open data sets include tutorial notebooks that provide basic and advanced walk-throughs for developers.

DAX notebooks open in Watson Studio.

Machine learning (ML) uses algorithms – also known as “models” – to identify patterns in the data. 

Types of ML are Supervised, Unsupervised, and Reinforcement. 

Supervised learning comprises two types of models, regression and classification.

Deep learning refers to a general set of models and techniques that loosely emulate the way the human brain solves a wide range of problems.

The Model Asset eXchange is a free, open-source repository for ready-to-use and customizable deep-learning microservices.

MAX model-serving microservices are built and distributed on GitHub as open-source Docker images.

You can use Red Hat OpenShift, a Kubernetes platform, to automate deployment, scaling, and management of microservices.

Ml-exchange.org has multiple predefined models.



Jupyter Notebooks are used in Data Science for recording experiments and projects.

Jupyter Lab is compatible with many files and Data Science languages.

There are different ways to install and use Jupyter Notebooks.

How to run, delete, and insert a code cell in Jupyter Notebooks.

How to run multiple notebooks at the same time.

How to present a notebook using a combination of Markdown and code cells.

How to shut down your notebook sessions after you have completed your work on them.

Jupyter implements a two-process model with a kernel and a client.

The notebook server is responsible for saving and loading the notebooks.

The kernel executes the cells of code contained in the Notebook. 

The Jupyter architecture uses the NB convert tool to convert files to other formats.

Jupyter implements a two-process model with a kernel and a client.

The Notebook server is responsible for saving and loading the notebooks.

The Jupyter architecture uses the NB convert tool to convert files to other formats.

The Anaconda Navigator GUI can launch multiple applications on a local device.

Jupyter environments in the Anaconda Navigator include JupyterLab and VS Code.

You can download Jupyter environments separately from the Anaconda Navigator, but they may not be configured properly.

The Anaconda Navigator GUI can launch multiple applications.

Additional open-source Jupyter environments include JupyterLab, JupyterLite, VS Code, and Google Colaboratory. 

JupyterLite is a browser-based tool.